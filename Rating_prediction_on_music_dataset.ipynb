{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rating prediction on music dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4V0+4SPRV36BhdzG6n8zO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srishtysuman/NLP/blob/master/Rating_prediction_on_music_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOefHJY10G6I"
      },
      "source": [
        "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEy1iGlP0VZN"
      },
      "source": [
        "import gensim\n",
        "word_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Mm94gQ6wnYb"
      },
      "source": [
        "#1.0 = 2506, #2.0=2712, #3.0=6108, #4.0=14926,#5.0=31983\n",
        "\n",
        "Classes are highly imbalanced. So don't use accuracy as a measure. use confusion matrix or recall or precision or f1-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba58bYHRxJni"
      },
      "source": [
        "### function for loading files ###\n",
        "\n",
        "def load_doc(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.readlines()\n",
        "    file.close()\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEoYYV0mx8dS"
      },
      "source": [
        "#### loading files ####\n",
        "\n",
        "texts_x=load_doc('digital_music_5.x.train.txt')\n",
        "texts_y=load_doc('digital_music_5.y.train.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDU9mazwXP54"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "file1 = open('digital_music_5.y.train.txt', 'r') \n",
        "Lines = file1.readlines() \n",
        "count = 0\n",
        "target1=[]\n",
        "for line in Lines: \n",
        "    target1.append(float(line[0:-1]))\n",
        "#le = LabelEncoder()\n",
        "#le.fit([\"1.0\\n\", \"2.0\\n\",\"3.0\\n\",\"4.0\\n\",\"5.0\\n\"])\n",
        "#list(le.classes_)\n",
        "#labels=le.transform(target1)\n",
        "target1[:290]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ycvr4u6by2lq"
      },
      "source": [
        "#### tokenize the texts #####\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "# tokenize\n",
        "def tokenizer_(texts):\n",
        "  texts = [w.lower() for w in texts]\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  return sequences, tokenizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM1PNwFj7lXc"
      },
      "source": [
        "### tokenizing the train x ####\n",
        "\n",
        "train_tokenized, tokenizer=tokenizer_(texts_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmNEy5mC8jNf"
      },
      "source": [
        "train_tokenized is the tokenized version of texts_x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lthGjuVI32c",
        "outputId": "46732e60-8bfc-43d6-cc11-0dcf61b25a9e"
      },
      "source": [
        "## getting the work index from tokenizer #####\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 129726 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cd4c1no1NfEk",
        "outputId": "22ae713e-8738-424e-b542-b13d3734a683"
      },
      "source": [
        "#### obtaining the maximum length of a sequence in tokenized sequence ########\n",
        "\n",
        "maxl=0\n",
        "for x in train_tokenized:\n",
        "  t=len(x)\n",
        "  if(t>maxl):\n",
        "    maxl=t\n",
        "print(maxl)\n",
        "MAX_SEQUENCE_LENGTH=maxl\n",
        "EMBEDDING_DIM=100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIpwTjA7N4F3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1f0a6f8-bd68-440f-afec-becbe8fccede"
      },
      "source": [
        "## pad trained tokenized sequences with 0 to obtain same length sequences ######\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "data = pad_sequences(train_tokenized, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print(data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(58235, 4518)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNXrLNFjVOyw"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "train_x=data\n",
        "train_y=target1\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, test_size=0.20, random_state=42)\n",
        "x_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "y_train=tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
        "x_val = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
        "y_val=tf.convert_to_tensor(Y_val, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM6OAouO8zC5"
      },
      "source": [
        "from gensim.models import Word2Vec \n",
        "from gensim.models import KeyedVectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhpRHcQKUS6F"
      },
      "source": [
        "######### embedding index is a dictionary which contains vectors for each word in pretrained #####\n",
        "import numpy as np\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcGwa8SpUYiF"
      },
      "source": [
        "########### embedding matrix containing vectors corresponding to each word ###########\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        if(len(embedding_vector)<100):\n",
        "          embedding_vector += [0.0] * (100 - len(embedding_vector))\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3WxdASKY1LG"
      },
      "source": [
        "embedding_matrix[122000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NJ4AbXnNQET"
      },
      "source": [
        "######### Embedding layer ###########\n",
        "\n",
        "from keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyvGlL_eYcyC"
      },
      "source": [
        "from keras import Input\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dense, Flatten\n",
        "from keras import Model\n",
        "import keras\n",
        "from keras import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB58QDKkgkRI",
        "outputId": "1a6dc0ad-0651-4159-b259-eec76a2ef646"
      },
      "source": [
        "len(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46588"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvdjPppPuGPP"
      },
      "source": [
        "#### Model Definition #####\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Conv1D(128, 5, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4158, activation='relu'))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')\n",
        "##keras.metrics.Recall()\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_82JWitplGuc"
      },
      "source": [
        "# Fit the model\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "history = model.fit([train_x], y=to_categorical(train_y), batch_size=128, verbose=1, validation_split=0.25, shuffle=True, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55PMxaVelPMx"
      },
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(x_val, y_val, verbose=0)\n",
        "scores=[4]*len(x_val)\n",
        "print(\"Recall: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OgifWt4lTne"
      },
      "source": [
        "yp=np.argmax(model.predict(x_val), axis=-1)\n",
        "print(yp[:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q9hryYCUaN1"
      },
      "source": [
        "from sklearn import metrics\n",
        "matrix = metrics.confusion_matrix(y_val, yp)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycfJFZxDUoFr"
      },
      "source": [
        "print(matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0KZEzsChuw0"
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miQXAyOThYe8"
      },
      "source": [
        "estimator = KerasClassifier(build_fn = model, epochs = 10, batch_size = 10, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_tvFmJ-hkqn"
      },
      "source": [
        "kfold = KFold(n_splits = 5, shuffle = True, random_state = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KtCNI40hpfq"
      },
      "source": [
        "# Object to describe the result\n",
        "results = cross_val_score(estimator, train_x, train_y, cv = kfold)\n",
        "# Result\n",
        "print(\"Result: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2BHIbtIzW6p"
      },
      "source": [
        "print(yp[:800])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnEKpbUnYtR5"
      },
      "source": [
        "inputs = Input(shape=(MAX_LENGTH, ))\n",
        "\n",
        "x = LSTM(64)(embedding_layer)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "predictions = Dense(num_class, activation='softmax')(x)\n",
        "model = Model(inputs=[inputs], outputs=predictions)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}